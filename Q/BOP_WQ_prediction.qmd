---
title: "Predicting Enterococci Levels in NYC Harbor"
author: "Art Steinmetz"
date: "2024-07-20"
format: typst
execute:
  message: false
  error: true
  warning: false
  eval: true
  echo: false
---

![](img/3d_confusion.png)

```{r load libraries, message = FALSE, warning = FALSE, echo = FALSE}
library(tidyverse)
library(tidymodels)
library(duckplyr)
library(skimr)
library(gt)
library(here)
source(here("R/pretty_truth_table.R"))
rlang::local_options(duckdb.materialize_message = FALSE)
wq <- df_from_parquet(here("data/wq_model_data.parquet")) |>
  # remove rows with missing data
  remove_missing() |>
  as_tibble()
wq_data_4 <- df_from_parquet(here("data/wq_data_4.parquet")) |>
  as_tibble()

methods_restore()

# quality labels
SAFE <- 35
DANGER <- 104

```

## Abstract

This document explores the relationship between testing location,weather, tides and water quality in the NYC Harbor. The data sources are the Billion Oyster Project (BOP), the Citizens' Water Quality Testing Program and the NOAA.

This note is an update of a previous exploration of the data where I showed that a linear regression model was a very poor explainer of bacteria levels in New York Harbor water. In this note I set a lower bar by classifying bacteria into just three classes, "safe," (\<=35 colonies) "caution" (\<= 104 colonies) and "unsafe" (\> 104 colonies). I train a "random forest" machine learning model on a sub-sample of the data and then evaluate the model with a different test set of data.

In summary, this model works very well in fitting the training set but does much worse out of sample. The model does show good accuracy in predicting "safe" and "unsafe" water but very little accuracy in predicting bacteria levels in the "caution" range. The dominant predictor is the testing site, since several sites NEVER have "safe" water in the data set. No other variable stands out in significance.

This is not an academic-quality study. It is an exploration of the data. I am not a water quality expert or a professional statistician. Comments and criticism are welcome.

## Data

The main data source is the BOP water quality spreadsheet found here: [[BOP Water Quality Data]{.underline}](https://docs.google.com/spreadsheets/d/1813b2nagaxZ80xRfyMZNNKySZOitro5Nt7W4E9WNQDA/edit?gid=1924583806#gid=1924583806)[^1] I also used the NOAA data site for tide, temperature and rainfall data.

[^1]: <https://docs.google.com/spreadsheets/d/1813b2nagaxZ80xRfyMZNNKySZOitro5Nt7W4E9WNQDA/edit?gid=1924583806#gid=1924583806>

## Feature Engineering

The BOP data includes time of last high tide. I thought I could get more granular by imputing the direction and strength of the tidal current at the time of the water sample. I used the NOAA tide data to find the previous slack tide time and level, then the next slack tide time and level.By determining where in the tide phase the sample was taken and the total change in water level for that phase, I impute the direction and strength of the tidal current when the sample was taken using this formula:

$$
CurrentSpeed = HighLowRangeFt * sin(\pi * \frac{HoursSinceLastTide}{TideDurationHrs})
$$

So the further we are from a slack tide, high or low, the faster the current will be. The bigger the change in water level during a tidal phase, the stronger the current will be. Ebb tides are negative values, flood tides are positive. *CurrentSpeed* is an index so the units don't have a specific meaning like feet-per-second.

I get the tides from the closest NOAA tide station to each water sampling site. Where the location of the sampling site is not known, I default to the Battery tide station at the bottom of Manhattan. This occurs when the name of the sampling site does not agree with any site name in the location meta data. **There are significant number of such cases.**

The city of New York uses 48-hour rainfall amounts in its safety criteria so that is what I use as the precipitation variable.

The BOP data does not include temperature. I used the NOAA Central Park air temperature for each sample day as a data feature. This is a (not very good) proxy for the water temperature but also for seasonality. This allows seasonality to be a continuous variable. Otherwise, "month" would be a categorical variable.

In the end I chose to the following features: `Site`, `TideHighLowRange`, `HoursSinceLastTide`, `CurrentSpeed`, `48-HourPrecip` and `Temperature`.

## Data Exploration

The bacteria levels are distributed in a lopsided way. The extreme high level is effectively infinity and conveys little information. Values above 5000 are only 5% of the observations and values below 500 are 82% of the observations.

```{r bacteria_hist}

wq_rf_prep_cl <- wq |>
  ungroup() |>
  # create a 3 level factor
  mutate(bacteria_category = cut(bacteria, breaks = c(0, 35,104, Inf), labels = c("SAFE", "CAUTION", "UNSAFE"))) |>
  select(-bacteria) |>
  select(-precip_noaa) |>
  select(-precip_wk) |>
  separate(site,into = "body",remove = FALSE,extra = "drop") |>
  # convert all char columns to factors
  mutate_if(is.character,as.factor) |>
  # select(-site) |>
  select(-body) |>
  drop_na()

wq |> pivot_longer(bacteria) |>
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "lightblue") +
  labs(title = "Distribution of Bacteria Concentrations",
       x = "Bacteria Concentration",
       y = "Frequency") +
  theme_minimal()

```

If we group the bacteria levels according to the official quality standards we get a better behaved distribution.

```{r class_ist}
wq_rf_prep_cl |>
  group_by(bacteria_category) |>
  summarise(n = n()) |>
  ggplot(aes(x = bacteria_category, y = n)) +
  geom_col(fill = "lightblue") +
  labs(title = "Distribution of Bacteria Categories",
       x = "Bacteria Category",
       y = "Frequency") + 
  theme_minimal()

```

What are the distributions of all the variables? Note the tide level distribution are the levels at just the high and low tides.

```{r hist_all}
# plot histograms of all variables using ggplot
wq_rf_prep_cl |>
  ungroup() |>
  select(-site,-bacteria_category) |>
  gather() |>
  ggplot(aes(x = value)) +
  geom_histogram(bins=20,fill = "lightblue") +
  facet_wrap(~key, scales = "free") + 
  theme_minimal()
```

What are the cleanest and most contaminated sites?

```{r dirtiest}
median_all <- median(wq$bacteria)
# show a boxplot of bacteria concentration by site
wq |>
  # avoid Inf log values
  # mutate(bacteria = bacteria + .001) |> 
  group_by(site) |>
  nest() |>
  rowwise() |>
  mutate(n_obs = nrow(data)) |> 
  filter(n_obs > 100) |> 
  mutate(median_bacteria = median(data$bacteria)) |> 
  ungroup() |>
  slice_max(median_bacteria,n = 10) |>
  unnest(data) |>
  ggplot(aes(x = reorder(factor(site),median_bacteria), y = bacteria)) +
  scale_y_log10(oob = scales::squish_infinite) +
  geom_boxplot(fill = "lightblue") + 
  # geom_violin(draw_quantiles = .5) +
  # geom_jitter(width = .1) +
  # annotate("text", x = log(SAFE)-2, y = 1500, label = "Safe Levels", color = "darkgreen") +

  labs(title = "Dirtiest Sites",
       subtitle = "Based on Median Bacteria Count",
       x = "Site",
       y = "Boxplot of Enterococci Concentration\n(Log Scale)") +
  coord_flip() + 
  geom_hline(yintercept = SAFE,color="red",linewidth=2) + 
  annotate("label", x = 9, y = SAFE, label = "Safe\nLevel", color = "red") +
  theme_minimal()

```

```{r cleanest}
#best sites
wq |>
  # avoid Inf log values
  # mutate(bacteria = bacteria + .001) |> 
  group_by(site) |>
  nest() |>
  rowwise() |>
  mutate(n_obs = nrow(data)) |> 
  filter(n_obs > 100) |> 
  mutate(median_bacteria = median(data$bacteria)) |> 
  ungroup() |>
  slice_min(median_bacteria,n = 10) |>
  unnest(data) |>
  ggplot(aes(x = reorder(factor(site),median_bacteria), y = bacteria)) +
  scale_y_log10(oob = scales::squish_infinite) +
  geom_boxplot(fill = "lightblue") + 
  # geom_violin(draw_quantiles = .5) +
  # geom_jitter(width = .1) +
  # annotate("text", x = log(SAFE)-2, y = 1500, label = "Safe Levels", color = "darkgreen") +

  labs(title = "Cleanest Sites",
       subtitle = "Based on Median Bacteria Count",
       x = "Site",
       y = "Boxplot of Enterococci Concentration\n(Log Scale)") +
  coord_flip() + 
  geom_hline(yintercept = SAFE,color="red",linewidth=2) + 
  annotate("label", x = 10, y = SAFE, label = "Safe\nLevel", color = "red") + 
  theme_minimal()
```

What is obvious is that even the cleanest sites have a lot of variation in bacteria levels. This might give us some hope that environmental factors might be more important than location in predicting bacteria levels.

Now let's look at some trends over time. Sadly, the overall level of bacteria has not improved over time. Looking at temperature, there are no clear trends. There are a couple years where a lot of rainfall seems associated with more bacteria but other years contradict that.

```{r}
rain_axis = 2.5
wq_data_4 |>
  filter(year > 2011) |>
  summarise(.by = year, median_bacteria = median(bacteria),
            median_temp = median(temperature_noaa),
            "total_rainfall" = mean(precip_wk)*20*rain_axis) |>
  ggplot(aes(x = year, y = median_bacteria)) + geom_col(fill = "lightblue") +
  # geom_smooth(aes(x = year, y = median_bacteria),color = "black",se = FALSE) +
  geom_line(aes(x = year, y = median_temp), color = "red") +
  geom_line(aes(x = year, y = total_rainfall), color = "blue") +
  # label the y-axes
  scale_x_continuous(breaks = seq(2000,2024,1)) +
  # put totat_rainfall on secondary y-axis
  scale_y_continuous(sec.axis = sec_axis(~./rain_axis, name = "Total Rainfall (Blue Line")) +
  labs(y = "Median Bacteria Concentration\nand Temperature (Red Line)",
       title= str_to_title("water is not getting cleaner over time"),
       subtitle = "May-September, 2012-2024") + 
  theme_minimal()

```

## Modeling

We use a random forest algorithm to train a prediction model. This class of models works very well on imbalanced data like we have here. It can also handle data sets with many categorical inputs like `site` in this case. More on this technique can be found at [<https://en.wikipedia.org/wiki/Random_forest>]{.underline} . To create the model we split the data randomly into a training set and a test set. 75% is used for training and the rest we hold out for testing. The sets are stratified so the same proportion of each bacteria category is in each set. The model is tuned using cross-validation on the training set and then evaluated on the test set.

## Results

The simplest way to evaluate the model is to look at the confusion matrix. This is a table that shows the number of correct and incorrect predictions for each category. In a perfect model all the observations would lie on the diagonal and the off-diagonal counts would all be zero. The table below shows that out of 1150 "UNSAFE" observations, the model predicted 903, or 81%, correctly. This was the best result. On the other hand, in 7% of *all* the cases, the model predicted the water was "SAFE" when the actual was "UNSAFE" (176/2544). Additionally, The model is far better at predicting "SAFE" and "UNSAFE" than "CAUTION."

```{r}
# show a confusion matrix
load(here("data/wq_pred_final.rdata"))
xt <- wq_pred_final |>
  conf_mat(truth = bacteria_category, estimate = .pred_class) |>
  # return just the confusion matrix
  pluck("table") |>
  as_tibble() |>
  group_by(Truth) |>
  mutate(.prop = n/sum(n)) |>
  ungroup()

xt_count <- xt |>
  select(-.prop) |>
  pivot_wider(names_from = Prediction,values_from = n) |>
  rowwise() |>
  mutate(Total = sum(c(SAFE,CAUTION,UNSAFE)))

gt_domain <- xt_count |> select(-Total,-Truth) |> max()

xt_prop <- xt |>
  select(-n) |>
  pivot_wider(names_from = Prediction,values_from = .prop) |>
  rowwise() |>
  mutate(Total = sum(c(SAFE,CAUTION,UNSAFE)))

# truth_table(xt_prop,"prop")
truth_table(xt_count,"count")
```

In a regression analysis, we measure the coefficient of each of the input variables so there is a precise measure of how much each variable contributes to the prediction. In a random forest, we don't have linear relationships but we can measure the relative importance of each variable.

The `site` input is nearly twice as important as the other inputs. Since location doesn't change over time, modeling might seem superfluous. Overall results can be summarized with the the single *"Kappa"* statistic which indicates the model is about 30 percentage points better than random chance. If we remove the `site` input from the model, the prediction validity is reduced, as we'd expect, but it is still 20% better than random chance. In such a model 2-day rainfall becomes the most important input.

```{r}

# show variable importance
load(here("data/var_imp.rdata"))

var_imp |> 
  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "lightblue") +
  coord_flip() +
  labs(title = "Water Quality Variable Importance",
       subtitle = 'Contribution to Random Forest Classifier\nto Predict "SAFE", "CAUTION" or "UNSAFE ',
       x = "Variable") +
  theme_minimal()
```

The "Receiver Operator Curve" visualizes how much better than model is than random chance. Curves that bend up and to the left are better. We can see that the "CAUTION" predictions are barely better than a coin flip.

```{r}
wq_pred_final |>
  roc_curve(truth = bacteria_category, .pred_SAFE,.pred_CAUTION,.pred_UNSAFE) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity,color=.level)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "Is The Model Better Than Random Chance?",
       subtitle = "ROC Curve for Random Forest Classifier",
       x = "1 - Specificity",
       y = "Sensitivity",
       color = "Prediction") +
  scale_color_manual(values = c("orange", "green","red")) +
  # annotate to label better and worse than chance
  annotate("text",label = "Random Chance",
           x = .5, y = .5,
           color = "black",
           angle = 45, hjust = .5, vjust = 1.5) +
  theme_bw()

```

Every prediction the model makes includes a confidence level. The classification with the highest confidence level is what the model predicts. How certain is the model that the prediction is correct? The plot below shows the confidence level for each prediction. The majority of the classifications are correct but we can see the model is highly confident in many cases where it is wrong.

```{r}
# plot prediction confidence
wq_pred_final |>
  rename(Actual_Category = bacteria_category) |>
  ggplot(aes(x = .pred_SAFE, y = .pred_UNSAFE, color = Actual_Category)) +
  facet_wrap(~Actual_Category) +
  geom_point() +
  geom_abline() +
  scale_color_manual(values = c("green", "orange","red")) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Model Prediction Confidence",
       x = "Confidence Actual Is SAFE",
       y = "Confidence Actual Is UNSAFE")

```

::: callout-note
We can illustrate the importance of separating training and testing data. It's easy to "overfit" when including all of the data. In the example below we train on all of the water quality data. The accuracy and prediction confidence is very high but it's an illusion. We don't know anything about predictive ability in the future.

## Prediction is "Easy" When We Overfit

```{r}
# loads wq_pred_cl
load(here("data/wq_overfit_cl.rdata"))

# plot prediction confidence
xt <- wq_pred_cl |>
  conf_mat(truth = bacteria_category, estimate = .pred_class) |>
  # return just the confusion matrix
  pluck("table") |>
  as_tibble() |>
  group_by(Truth) |>
  mutate(.prop = n/sum(n)) |>
  ungroup()

xt_count <- xt |>
  select(-.prop) |>
  pivot_wider(names_from = Prediction,values_from = n) |>
  rowwise() |>
  mutate(Total = sum(c(SAFE,CAUTION,UNSAFE)))

gt_domain <- xt_count |> select(-Total,-Truth) |> max()

xt_prop <- xt |>
  select(-n) |>
  pivot_wider(names_from = Prediction,values_from = .prop) |>
  rowwise() |>
  mutate(Total = sum(c(SAFE,CAUTION,UNSAFE)))

# truth_table(xt_prop,"prop")
truth_table(xt_count,"count")

```
:::

## Conclusion

We have created a random forest model that shows modest accuracy in predicting whether enterococci levels will be at the extremes of "safe" or "unsafe." The model is not very good at predicting the middle "caution" levels of bacteria. Unfortunately, the site location itself is the most important predictor of bacteria levels so changing environmental factors like tide, temperature and rainfall tell us little about levels over time. Understanding point sources of pollution around each site and how they vary might be more fruitful.
