---
title: "Predicting Enterococci Levels in New York Harbor"
author: "A Study for the Billion Oyster Project by Arthur Steinmetz"
date: "2024-08-07"
format: typst
execute:
  message: false
  error: true
  warning: false
  eval: true
  echo: false
---

![](img/3d_confusion.png)

```{r load libraries, message = FALSE, warning = FALSE, echo = FALSE}
library(tidyverse)
library(tidymodels)
library(duckplyr)
library(skimr)
library(gt)
library(here)
source(here("R/func_pretty_truth_table.R"))
rlang::local_options(duckdb.materialize_message = FALSE)
wq <- df_from_parquet(here("data/wq_model_data.parquet")) |>
  # remove rows with missing data
  remove_missing() |>
  as_tibble() |>
  mutate(across(where(is.character),as.factor)) |>
  # re-order quality factor levels
  mutate(quality = fct_relevel(quality, "Safe", "Caution", "Unsafe")) |>
  # use our weather, not BOP's
  select(-starts_with("bop")) 

methods_restore()

# quality labels
SAFE <- 35
DANGER <- 104

```

## Abstract

This document explores the relationship between testing location, weather, tides and water quality in the NYC Harbor. The data sources are the Billion Oyster Project (BOP), the Citizens' Water Quality Testing Program and the NOAA. The data spans from Autumn 2011 to June 27, 2024. The stations reporting vary over time.

This note is an update of a previous exploration of the data where I showed that a linear regression model was a very poor explainer of bacteria levels in New York Harbor water. In this note I set a lower bar by classifying bacteria into just three classes, "safe," (\<=35 colonies) "caution" (\<= 104 colonies) and "unsafe" (\> 104 colonies). I train a "random forest" machine learning model on a sub-sample of the data and then evaluate the model with a different test set of data.

In summary, this model works very well in fitting the training set but does much worse out of sample. The model does show good accuracy in predicting "safe" and "unsafe" water but very little accuracy in predicting bacteria levels in the "caution" range. The dominant predictor is the testing site, since several sites NEVER have "safe" water in the data set. No other variable stands out in significance.

This is not an academic-quality study. It is an exploration of the data. I am not a water quality expert or a professional statistician. Comments and criticism are welcome.

## Data

The main data source is the BOP water quality spreadsheet found here: [[BOP Water Quality Data]{.underline}](https://docs.google.com/spreadsheets/d/1813b2nagaxZ80xRfyMZNNKySZOitro5Nt7W4E9WNQDA/edit?gid=1924583806#gid=1924583806)[^1] I also used the NOAA data site for tide, temperature and rainfall data.

[^1]: <https://docs.google.com/spreadsheets/d/1813b2nagaxZ80xRfyMZNNKySZOitro5Nt7W4E9WNQDA/edit?gid=1924583806#gid=1924583806>

## Feature Engineering

The BOP data includes time of last high tide. I thought I could get more granular by imputing the direction and strength of the tidal current at the time of the water sample. I used the NOAA tide data from the nearest statoin to find the previous slack tide time and level, then the next slack tide time and level. By determining where in the tide phase the sample was taken and the total change in water level for that phase, I impute the direction and strength of the tidal current when the sample was taken using this formula:

$$
CurrentSpeed = HighLowRangeFt * sin(\pi * \frac{HoursSinceLastTide}{TideDurationHrs})
$$

So the further we are from a slack tide, high or low, the faster the current will be. The bigger the change in water level during a tidal phase, the stronger the current will be. Ebb tides are negative values, flood tides are positive. *CurrentSpeed* is an index so the units don't have a specific meaning like feet-per-second.

The city of New York uses 48-hour rainfall amounts in its safety criteria so that is what I use as one precipitation variable. Since the sample time is in the middle of the day, that day's rainfall might not be relevant. I choose the prior two days for the 48-hour period. I also use the current day rainfall so there are three days of precipitation in the dataset. I use the NOAA rainfall data for the closest station to the sampling site. Where the location of the sampling site is not known, I default to the Central Park weather station. Station location is more important than it might first appear. Rainfall varies widely by location in the city.

```{r}
# use full weather history
precip_year <-arrow::read_parquet(here("data/weather_ghcn.parquet")) |>
  select(date,ghcn_station_name,precip_in) |>
  mutate(year = lubridate::year(date)) |>
  summarise(.by = c(year,ghcn_station_name),
            n = n(),
            precip = mean(precip_in, na.rm = TRUE)*365) |>
  na.omit() |>
  # fewer observations make for less useful generalizations
  filter(year == 2023, n > 200)

precip_year |>
  ggplot(aes(x = factor(ghcn_station_name), y = precip,group = ghcn_station_name)) +
  geom_boxplot(color = "blue") +
  labs(title = "There Is Significant Variation In Annual\nRainfall Across Stations",
       subtitle = "Estimated Total Rainfall in 2023",
       x = "NOAA Stations With At Least\n200 Daily Observations in 2023",
       y = "Annualized Daily Rainfall (inches)") +
  theme_minimal() +
  coord_flip()

```

The BOP data does not include temperature. I used the NOAA Air temperature at the nearest station for each sample day as a data feature. This is a (not very good) proxy for the water temperature but also for seasonality. This allows seasonality to be a continuous variable. Otherwise, "month" would be a categorical variable but we can omit it.

The NOAA Weather and tide data I retrieved are substituted for the equivalent features in the BOP water quality spreadsheet. This is because many of the BOP observations default to either Central Park or Battery Park, irrespective of the closest NOAA station. This leaves us with the following data features:

```{r}
names(wq)
```

## Data Exploration

As mentioned above, the number of stations reporting varies over time. It has grown steadily though during the COVID crisis fewer stations reported.

```{r}
wq |>
  mutate(year = year(date)) |>
  group_by(year) |>
  summarise(n = n_distinct(site_id)) |>
  ggplot(aes(x = year, y = n)) +
  geom_col(fill = "lightblue") +
  labs(title = "Reporting Stations Have Steadily Increased",
       x = "Year",
       y = "Number of Stations") +
  theme_minimal()
```

The bacteria levels are distributed in a lopsided way. The extreme high level is effectively infinity and conveys little information. Values above 5000 are only 5% of the observations and values below 500 are 82% of the observations.

```{r bacteria_hist}

wq_rf_prep_cl <- wq |>
  ungroup() |>
  select(-date,-bacteria) |>
  select(-site_id) |>
  # mutate(site = as_factor(site)) |>
  # dimension reduction
  drop_na()


wq |> pivot_longer(bacteria) |>
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "lightblue") +
  labs(title = "Distribution of Bacteria Concentrations",
       x = "Bacteria Concentration",
       y = "Count") +
  theme_minimal()

```

If we group the bacteria levels according to the official quality standards we get a better behaved distribution. As a result, we will attempt to predict the quality classification of the water rather than the bacteria level.

```{r class_hist}
wq_rf_prep_cl |>
  group_by(quality) |>
  summarise(n = n()) |>
  ggplot(aes(x = quality, y = n)) +
  geom_col(fill = "lightblue") +
  labs(title = "Distribution of Bacteria Categories",
       x = "Water Quality",
       y = "Count") + 
  theme_minimal()

```

What are the distributions of all the variables? Note the tide level distribution shows the levels at just the high and low tides but we know when in the tidal phase the sample was taken.

```{r hist_all}
# plot histograms of all variables using ggplot
wq_rf_prep_cl |>
  ungroup() |>
  select(is.numeric) |>
  gather() |>
  ggplot(aes(x = value)) +
  geom_histogram(bins=20,fill = "lightblue") +
  facet_wrap(~key, scales = "free") + 
  labs(title = "Distribution of All Numeric Variables",
       y = "Count",x="") +
  theme_minimal()
```

The remaining features are categorical, not continuous variables, so we will have to take that into account when we build our model.

What are the cleanest and most contaminated sites? The boxes in the plots show the median and inter-quartile ranges.

```{r dirtiest}
median_all <- median(wq$bacteria)
# show a boxplot of bacteria concentration by site
wq |>
  # avoid Inf log values
  # mutate(bacteria = bacteria + .001) |> 
  group_by(site) |>
  nest() |>
  rowwise() |>
  mutate(n_obs = nrow(data)) |> 
  filter(n_obs > 100) |> 
  mutate(median_bacteria = median(data$bacteria)) |> 
  ungroup() |>
  slice_max(median_bacteria,n = 10) |>
  unnest(data) |>
  ggplot(aes(x = reorder(factor(site),median_bacteria), y = bacteria)) +
  scale_y_log10(oob = scales::squish_infinite) +
  geom_boxplot(fill = "lightblue") + 
  # geom_violin(draw_quantiles = .5) +
  # geom_jitter(width = .1) +
  # annotate("text", x = log(SAFE)-2, y = 1500, label = "Safe Levels", color = "darkgreen") +

  labs(title = "Dirtiest Sites",
       subtitle = "Based on Median Bacteria Count",
       x = "Site",
       y = "Boxplot of Enterococci Concentration\n(Log Scale)") +
  coord_flip() + 
  geom_hline(yintercept = SAFE,color="red",linewidth=2) + 
  annotate("label", x = 9, y = SAFE, label = "Safe\nLevel", color = "red") +
  theme_minimal()

```

```{r cleanest}
#best sites
wq |>
  # avoid Inf log values
  # mutate(bacteria = bacteria + .001) |> 
  group_by(site) |>
  nest() |>
  rowwise() |>
  mutate(n_obs = nrow(data)) |> 
  filter(n_obs > 100) |> 
  mutate(median_bacteria = median(data$bacteria)) |> 
  ungroup() |>
  slice_min(median_bacteria,n = 10) |>
  unnest(data) |>
  ggplot(aes(x = reorder(factor(site),median_bacteria), y = bacteria)) +
  scale_y_log10(oob = scales::squish_infinite) +
  geom_boxplot(fill = "lightblue") + 
  # geom_violin(draw_quantiles = .5) +
  # geom_jitter(width = .1) +
  # annotate("text", x = log(SAFE)-2, y = 1500, label = "Safe Levels", color = "darkgreen") +

  labs(title = "Cleanest Sites",
       subtitle = "Based on Median Bacteria Count",
       x = "Site",
       y = "Boxplot of Enterococci Concentration\n(Log Scale)") +
  coord_flip() + 
  geom_hline(yintercept = SAFE,color="red",linewidth=2) + 
  annotate("label", x = 10, y = SAFE, label = "Safe\nLevel", color = "red") + 
  theme_minimal()
```

What is obvious is that even the cleanest sites have a lot of variation in bacteria levels. This might give us some hope that environmental factors might be more important than location in predicting bacteria levels.

Now let's look at some trends over time. Sadly, the overall level of bacteria has not improved over time. Looking at temperature, there are no clear trends. There does seem to be a relationship between rainfall and bacteria. Note the rainfall measurements are from the closest NOAA site on the day of the water sample, so it may not be a proxy for annual rainfall.

As we saw, the number of reporting stations has increased so the data may not be comparable year-on-year. Let's look at just the stations that have been reporting for at least 10 years.

```{r}
wq_10 <- wq |> 
  # filter out stations that have been reporting for less than 10 years
  group_by(site) |>
  mutate(year = year(date)) |>
  summarise(n_years = n_distinct(year)) |>
  filter(n_years > 9) |>
  select(site) |>
  inner_join(wq, by = "site") |>
  ungroup() |> 
  filter(year(date) > year(Sys.Date())-11)
  
rain_axis = 200
last_obs <- wq$date |> max()
first_obs <- year(last_obs)-10

wq_10 |>
  ungroup() |>
  mutate(year = year(date),month = month(date)) |> 
  filter(year > 2011) |>
  summarise(.by = year, median_bacteria = median(bacteria),
            median_temp = median(temperature_f),
            average_rainfall = mean(ghcn_precip_in)*rain_axis) |>
  ggplot(aes(x = year, y = median_bacteria)) + geom_col(fill = "lightblue") +
  # geom_smooth(aes(x = year, y = median_bacteria),color = "black",se = FALSE) +
  geom_line(aes(x = year, y = median_temp), color = "red") +
  geom_line(aes(x = year, y = average_rainfall), color = "blue") +
  # label the y-axes
  scale_x_continuous(breaks = seq(2000,2024,1)) +
  # put totat_rainfall on secondary y-axis
  scale_y_continuous(sec.axis = sec_axis(~./rain_axis, name = "Average Daily Rainfall (Blue Line")) +
  labs(y = "Median Bacteria Concentration (Blue Bar)\nand Temperature (Red Line)",
       title= str_to_title("water is not getting cleaner over time"),
       subtitle = glue::glue("{first_obs} to {last_obs}")
       ) + 
  theme_minimal()
  
```

## Modeling

We use a random forest algorithm to train a prediction model. This class of models works very well on imbalanced data like we have here. It can also handle data sets with many categorical inputs like `site` in this case. More on this technique can be found at [[https://en.wikipedia.org/wiki/Random_forest]{.underline}](https://en.wikipedia.org/wiki/Random_forest){.uri} . To create the model we split the data randomly into a training set and a test set. 75% is used for training and the rest we hold out for testing. The sets are stratified so the same proportion of each bacteria quality is in each set. The model is tuned using cross-validation on the training set and then evaluated on the test set.

All of the code and data used to create the model can be found in the project's GitHub repository at [[https://github.com/apsteinmetz/oyster.git]{.underline}](https://github.com/apsteinmetz/oyster.git){.uri} .

## Results

The simplest way to evaluate the model is to look at the confusion matrix. This is a table that shows the number of correct and incorrect predictions for each quality. In a perfect model all the observations would lie on the diagonal and the off-diagonal counts would all be zero. The table below shows that out of 1194 "UNSAFE" observations, the model predicted or 76%, correctly. This was the best result. While overall accuracy is important, we might be most concerned about cases when the model predicts "SAFE" water when it's not. In 8% of *all* the cases, the model predicted the water was "SAFE" when the actual was "UNSAFE" (239/3082). Additionally, The model is far better at predicting "SAFE" and "UNSAFE" than "CAUTION."

::: {layout-ncol="2"}
```{r}
#| fig-cap: "Proportions"

# fitted model results come from wq_model.r
# show a confusion matrix
load(here("data/wq_pred_final.rdata"))
xt <- wq_pred_final |>
  conf_mat(truth = quality, estimate = .pred_class) |>
  # return just the confusion matrix
  pluck("table") |>
  as_tibble() |>
  group_by(Truth) |>
  mutate(.prop = n/sum(n)) |>
  ungroup()

xt_count <- xt |>
  select(-.prop) |>
  pivot_wider(names_from = Prediction,values_from = n) |>
  rowwise() |>
  mutate(Total = sum(c(Safe,Caution,Unsafe)))

gt_domain <- xt_count |> select(-Total,-Truth) |> max()

xt_prop <- xt |>
  select(-n) |>
  pivot_wider(names_from = Prediction,values_from = .prop) |>
  rowwise() |>
  mutate(Total = sum(c(Safe,Caution,Unsafe)))

truth_table(xt_prop,"prop")
```

```{r}
#| fig-cap: "Counts"
truth_table(xt_count,"count")
```
:::

In a regression analysis, we measure the coefficient of each of the input variables so there is a precise measure of how much each variable contributes to the prediction. In a random forest, we don't have linear relationships but we still can measure the relative importance of each variable.

Daily rainfall is the most important determinant of water quality. This is not surprising since heavy rainfall overwhelms the sewage system and washes bacteria into the water. The `site` input is the second most important feature. Since location doesn't change over time, we don't need a model to tell us some spots are worse than others. Somewhat surprisingly, then, `sewershed` is not particularly important. 

```{r}

# show variable importance
load(here("data/var_imp.rdata"))

var_imp |> 
  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "lightblue") +
  coord_flip() +
  labs(title = "Water Quality Variable Importance",
       subtitle = 'Contribution to Random Forest Classifier\nto Predict "SAFE", "CAUTION" or "UNSAFE ',
       x = "Variable") +
  theme_minimal()
```

The "Receiver Operator Curve" visualizes how much better than model is than random chance. Curves that bend up and to the left are better. We can see that the "Caution" predictions are barely better than a coin flip. Overall results can be summarized with the the single *"Kappa"* statistic which indicates the model is about 39 percentage points better than random chance.

```{r}
wq_pred_final |>
  roc_curve(truth = quality, .pred_Safe,.pred_Caution,.pred_Unsafe) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity,color=.level)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(title = "Is The Model Better Than Random Chance?",
       subtitle = "ROC Curve for Random Forest Classifier",
       x = "1 - Specificity",
       y = "Sensitivity",
       color = "Prediction") +
  scale_color_manual(values = c("orange", "green","red")) +
  # annotate to label better and worse than chance
  annotate("text",label = "Random Chance",
           x = .5, y = .5,
           color = "black",
           angle = 45, hjust = .5, vjust = 1.5) +
  theme_bw()

```

Every prediction the model makes includes a confidence level. The quality with the highest confidence level is what the model predicts. How certain is the model that the prediction is correct? The plot below shows the confidence level for each prediction. Each panel contains a dot for each observation, divided into the actual classifications.  The position of each dot shows the relative confidence the model has that the water is "Safe" and "Unsafe." The observations and confidence values for "Caution" are not shown.  The majority of the quality classifications are correct but we can see the model is highly confident in many cases where it is wrong.

```{r}
# plot prediction confidence
wq_pred_final |>
  rename(Actual_quality = quality) |>
  filter(Actual_quality != "Caution") |> 
  ggplot(aes(x = .pred_Safe, y = .pred_Unsafe, color = Actual_quality)) +
  facet_wrap(~Actual_quality) +
  geom_point(,size=.5) +
  geom_abline() +
  # scale_color_manual(values = c("green", "orange","red")) +
  scale_color_manual(values = c("green","red")) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Model Prediction Confidence",
       x = "Confidence Actual Is SAFE",
       y = "Confidence Actual Is UNSAFE")

```


## Conclusion

We have created a random forest model that shows modest accuracy in predicting whether enterococci levels will be at the extremes of "safe" or "unsafe." The model is not very good at predicting the middle "caution" levels of bacteria. We have shown that rainfall is the most important determinant of water quality. Unfortunately, the site location itself is an extremely important predictor of bacteria levels so changing environmental factors like tide and temperature tell us little about levels over time. Understanding point sources of pollution around each site and how they vary over time seems like a good next step but the fact that `sewershed` is not a very important predictor is discouraging.

::: callout-note
We can illustrate the importance of separating training and testing data. It's easy to "overfit" when including all of the data. In the example below we train on all of the water quality data. The accuracy and prediction confidence are very high but it's an illusion. We don't know anything about predictive ability in the future.

## Prediction is "Easy" When We Overfit

```{r}
# loads wq_pred_cl
load(here("data/wq_overfit_cl.rdata"))

# plot prediction confidence
xt <- wq_pred_cl |>
  conf_mat(truth = quality, estimate = .pred_class) |>
  # return just the confusion matrix
  pluck("table") |>
  as_tibble() |>
  group_by(Truth) |>
  mutate(.prop = n/sum(n)) |>
  ungroup()

xt_count <- xt |>
  select(-.prop) |>
  pivot_wider(names_from = Prediction,values_from = n) |>
  rowwise() |>
  mutate(Total = sum(c(Safe,Caution,Unsafe)))

gt_domain <- xt_count |> select(-Total,-Truth) |> max()

xt_prop <- xt |>
  select(-n) |>
  pivot_wider(names_from = Prediction,values_from = .prop) |>
  rowwise() |>
  mutate(Total = sum(c(Safe,Caution,Unsafe)))

truth_table(xt_prop,"prop")
# truth_table(xt_count,"count")

```
:::
